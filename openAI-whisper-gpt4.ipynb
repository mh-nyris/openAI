{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script tries to use user voice to chat with ChatGPT and gives voice to ChatGPTs response.\\nThis involves recording voice input or use recordings on disc towards a speech-to-text (stt) transformation. The resulting text serves as input to ChatGPT-4 via the openAI API. The text response from GPT-4 will then be used for a text-to-speech (tts) transformation (google-tts or gTTS).\\nAs last step this speech / audio signal is played back via a python launched audio player.\\n\\nSo Here is the code with the sound recording commented out (assuming here we already have a recording on disc called \"user_input_voice_recording.wav\"):\\n\\nimport openai\\nimport sounddevice as sd\\nfrom scipy.io.wavfile import write, read\\n\\naudio_file= open(\"user_input_voice_recording.wav\", \"rb\")\\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\\ntranscript[\"text\"]\\n\\ngpt_response = openai.ChatCompletion.create(\\n  model=\"gpt-4\",\\n  messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n        {\"role\": \"user\", \"content\": transcript[\"text\"]},\\n    ]\\n)\\n\\ngpt_res_text = gpt_response[\\'choices\\'][0][\\'message\\'][\\'content\\']\\n\\n# Print text response of ChatGPT\\nprint(gpt_res_text)\\n\\nfrom gtts import gTTS\\n\\n# Create a gTTS object with the text to be converted\\ngpt_res_t2s = gTTS(gpt_res_text)\\n\\n# Save the audio file\\ngpt_res_t2s.save(\"gpt-response-text-to-speech.mp3\")\\n\\n\\n# To do for GPT-4:\\n# Load the saved file \"gpt-response-text-to-speech.mp3\" and play them using suitable python package\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt to GPT-4\n",
    "\n",
    "\"\"\"\n",
    "This script tries to use user voice to chat with ChatGPT and gives voice to ChatGPTs response.\n",
    "This involves recording voice input or use recordings on disc towards a speech-to-text (stt) transformation. The resulting text serves as input to ChatGPT-4 via the openAI API. The text response from GPT-4 will then be used for a text-to-speech (tts) transformation (google-tts or gTTS).\n",
    "As last step this speech / audio signal is played back via a python launched audio player.\n",
    "\n",
    "So Here is the code with the sound recording commented out (assuming here we already have a recording on disc called \"user_input_voice_recording.wav\"):\n",
    "\n",
    "import openai\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write, read\n",
    "\n",
    "audio_file= open(\"user_input_voice_recording.wav\", \"rb\")\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "transcript[\"text\"]\n",
    "\n",
    "gpt_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": transcript[\"text\"]},\n",
    "    ]\n",
    ")\n",
    "\n",
    "gpt_res_text = gpt_response['choices'][0]['message']['content']\n",
    "\n",
    "# Print text response of ChatGPT\n",
    "print(gpt_res_text)\n",
    "\n",
    "from gtts import gTTS\n",
    "\n",
    "# Create a gTTS object with the text to be converted\n",
    "gpt_res_t2s = gTTS(gpt_res_text)\n",
    "\n",
    "# Save the audio file\n",
    "gpt_res_t2s.save(\"gpt-response-text-to-speech.mp3\")\n",
    "\n",
    "\n",
    "# To do for GPT-4:\n",
    "# Load the saved file \"gpt-response-text-to-speech.mp3\" and play them using suitable python package\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from scipy.io.wavfile import write, read\n",
    "\n",
    "# Set OpenAI's and Whisper's API keys\n",
    "openai.organization = \"org-iaAwuprljYaoOoSAlUNCnaJQ\"\n",
    "openai.api_key = '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'  # Replace with your OpenAI key\n",
    "#openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Record voice\n",
    "# import sounddevice as sd\n",
    "# from scipy.io.wavfile import write\n",
    "\n",
    "# samplerate = 16000  # Use a sample rate of 16KHz\n",
    "# duration = 10.0  # Record for 5 seconds\n",
    "# print(\"Please speak into the microphone\")\n",
    "# mydata = sd.rec(int(samplerate * duration), samplerate=samplerate,\n",
    "#                 channels=1, blocking=True)\n",
    "# print(\"Recording complete, converting speech to text...\")\n",
    "\n",
    "# # Save as WAV file\n",
    "# write('user_input_voice_recording.mp3', samplerate, mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, this is a test. Let's see what I can subscribe this with another pin API API an AI API\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file= open(\"user_input_voice_recording.wav\", \"rb\")\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "transcript[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Why did the flower break up with the pollen? It just couldn't stamen relationship any longer!\n",
      "\n",
      "2) What did the pistil say to the stamen at the pollination party? \"Stop pollen my leg!\"\n",
      "\n",
      "3) Why was the pollen always getting in trouble? Because it was always pistil-whipped!\n",
      "\n",
      "4) What do plants call a one-night-stand? A pollen-pass encounter!\n",
      "\n",
      "5) Why did the flower blush when it saw the bee? Because it was ovary-whelmed by its pollen proposals!\n"
     ]
    }
   ],
   "source": [
    "gpt_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": transcript[\"text\"]},\n",
    "    ]\n",
    ")\n",
    "\n",
    "gpt_res_text = gpt_response['choices'][0]['message']['content']\n",
    "\n",
    "# Print text response of ChatGPT\n",
    "print(gpt_res_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "\n",
    "# Create a gTTS object with the text to be converted\n",
    "gpt_res_t2s = gTTS(gpt_res_text)\n",
    "\n",
    "# Save the audio file\n",
    "gpt_res_t2s.save(\"gpt-response-text-to-speech.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# Load the audio file\n",
    "audio = AudioSegment.from_mp3(\"/Users/mikehemberger/Documents/vscode/openAI/gpt-response-text-to-speech.mp3\")\n",
    "\n",
    "# Play the audio file\n",
    "play(audio)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
